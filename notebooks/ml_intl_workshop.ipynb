{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2a2df1f",
   "metadata": {},
   "source": [
    "# Text Classification Project Lifecycle\n",
    "---------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582e0ba9",
   "metadata": {},
   "source": [
    "<img src=\"files/overview.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da5c3bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import bz2\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from os import path\n",
    "import os\n",
    "\n",
    "os.chdir('../')\n",
    "os.getcwd()\n",
    "\n",
    "import ace.pipelines.pipeline_text as pt\n",
    "import ace.pipelines.pipeline_feature as pf\n",
    "import ace.pipelines.pipeline_ml as pm\n",
    "import ace.pipelines.pipeline_deploy as dl\n",
    "import ace.pipelines.pipeline_compare as pc\n",
    "\n",
    "from ace.factories.ml_factory import MLFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "918cc2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path vars\n",
    "data_path = path.join('data',  'notti_nhs.csv')\n",
    "pickle_data_file_name='nhs.pkl.bz2'\n",
    "\n",
    "pickle_file = path.join('data', 'processed', pickle_data_file_name)\n",
    "exp_data_path = path.join('data', 'processed')\n",
    "\n",
    "experiment_path = 'exp_1'\n",
    "lr_classifier = 'LogisticRegression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e24fa36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Engineering\n",
    "\n",
    "nhsn_data = pd.read_csv(data_path)\n",
    "nhsn_data = nhsn_data.dropna()\n",
    "\n",
    "with bz2.BZ2File(pickle_file, 'wb') as pickle_file:\n",
    "    # we pickle the data object as data, targets\n",
    "    pkl_obj = nhsn_data[['improve', 'super']], list(nhsn_data['super'])\n",
    "    pickle.dump(pkl_obj, pickle_file, protocol=4, fix_imports=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d438241",
   "metadata": {},
   "source": [
    "# text pre-processing\n",
    "\n",
    "- Lower-case\n",
    "- Spell check\n",
    "- Split joined-up words\n",
    "- Stopwords\n",
    "- Lemmatize\n",
    "- Stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893108c5",
   "metadata": {},
   "source": [
    "<img src=\"files/text_pipe.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48341a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting pipeline for improve\n",
      "Lower casing\n",
      "correcting spelling\n",
      "Finding joined up words\n",
      "removing stopwords\n",
      "Saving text pipeline\n",
      "Loading text pipeline for improve\n",
      "Transforming pipeline for improve\n",
      "Lower casing\n",
      "correcting spelling\n",
      "Finding joined up words\n",
      "removing stopwords\n",
      "Stemming using porter stemmer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[                                                 improve\n",
       " 0                                                   noth\n",
       " 1                                  DO not need TO improv\n",
       " 2                                                   noth\n",
       " 3                                                   noth\n",
       " 4      IT can BE difficult TO know the date the self ...\n",
       " ...                                                  ...\n",
       " 51838                             AN excel servic receiv\n",
       " 51839                                               noth\n",
       " 51840  I had TO wait approxim 3 month TO see A psychi...\n",
       " 51841                                   noth good servic\n",
       " 51842  I can not suggest ani improv all the care supp...\n",
       " \n",
       " [51843 rows x 1 columns]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.configure_pipeline(experiment_path,exp_data_path , spell=True, split_words=True, text_headers=['improve'],\n",
    "                      stop_words=True, lemmatize=False, stemm=True)\n",
    "\n",
    "pipe_text = pt.PipelineText(experiment_path, pickle_data_file_name)\n",
    "pipe_text.fit_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94ad084",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "- Count Matrix / TFIDF\n",
    "- Embeddings\n",
    "    - BERT\n",
    "    - sBERT\n",
    "    - w2v\n",
    "- Word/Doc Features\n",
    "    -word_counts\n",
    "    -NMF\n",
    "    -POS (Part of Speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed0dc9b",
   "metadata": {},
   "source": [
    "<img src=\"files/feature_pipe.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad41437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.configure_pipeline(experiment_path, feature_set=['frequency_matrix'], num_features=0, idf=True,\n",
    "                       feature_selection_type='Logistic', min_df=3, min_ngram=1, max_ngram=3)\n",
    "\n",
    "pipe_features = pf.PipelineFeatures(experiment_path, pickle_data_file_name)\n",
    "pipe_features.fit_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30a6720",
   "metadata": {},
   "source": [
    "<img src=\"files/tfidf.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b22868",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "- Logistic Regression\n",
    "- Random Forrests\n",
    "- Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb304c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pm.configure_pipeline(experiment_path)\n",
    "\n",
    "# ---- MODEL 1 --- #\n",
    "cls = MLFactory.factory(lr_classifier)\n",
    "pipe_ml = pm.MLTrainTest(experiment_path, pickle_data_file_name, classifier=cls)\n",
    "pipe_ml.fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5328b55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- MODEL 2 --- #\n",
    "rf_classifier = \"RandomForestClassifier\"\n",
    "cls2 = MLFactory.factory(rf_classifier)\n",
    "pipe_ml = pm.MLTrainTest(experiment_path, pickle_data_file_name, classifier=cls2)\n",
    "pipe_ml.fit_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d153d",
   "metadata": {},
   "source": [
    "# Deploy\n",
    "- Records for all pipeline steps are kept on __exp_1__ diractory on several config files\n",
    "- The model can easily be deployed on new unknown data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e237adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "responces_path = path.join('data',  'responces.csv')\n",
    "pickle_responces_file_name='responces.pkl.bz2'\n",
    "pickle_responces_file = path.join('data', 'processed', pickle_responces_file_name)\n",
    "\n",
    "responce_data = pd.read_csv(responces_path)\n",
    "\n",
    "with bz2.BZ2File(pickle_responces_file, 'wb') as pickle_file:\n",
    "    pkl_obj = responce_data[['improve']], None\n",
    "    pickle.dump(pkl_obj, pickle_file, protocol=4, fix_imports=False)\n",
    "\n",
    "pipe_text = pt.PipelineText(experiment_path, pickle_responces_file_name)\n",
    "# notice we do not call fit here!\n",
    "pipe_text.transform()\n",
    "\n",
    "pipe_features = pf.PipelineFeatures(experiment_path, pickle_responces_file_name)\n",
    "# notice we do not call fit here either!\n",
    "pipe_features.transform()\n",
    "\n",
    "dl.configure_pipeline(experiment_path, classifier_name, validation_path=experiment_path + \"/features\")\n",
    "ml_dep = dl.MLDeploy(experiment_path, pickle_responces_file_name)\n",
    "\n",
    "y_pred, y_prob = ml_dep.transform()\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8de3a80",
   "metadata": {},
   "source": [
    "# Change Features to sBERT!\n",
    "\n",
    "- Need to redo the text processing as stopwords removal, stemming etc, are not applicable with sBert\n",
    "- Need to redo the features pipeline asking for sBERT to be the main feature\n",
    "- Then run the rest of the pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2237c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.configure_pipeline(experiment_path,exp_data_path , spell=True, split_words=True, text_headers=['improve'],\n",
    "                      stop_words=False, lemmatize=False, stemm=False)\n",
    "\n",
    "pipe_text = pt.PipelineText(experiment_path, pickle_data_file_name)\n",
    "pipe_text.fit_transform()\n",
    "\n",
    "pf.configure_pipeline(experiment_path, feature_set=['sbert'])\n",
    "\n",
    "pipe_features = pf.PipelineFeatures(experiment_path, pickle_data_file_name)\n",
    "pipe_features.fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7db571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- MODEL 1 --- #\n",
    "cls = MLFactory.factory(lr_classifier)\n",
    "pipe_ml = pm.MLTrainTest(experiment_path, pickle_data_file_name, classifier=cls)\n",
    "pipe_ml.fit_transform()\n",
    "\n",
    "# ---- MODEL 2 --- #\n",
    "second_classifier_name = \"RandomForestClassifier\"\n",
    "cls2 = MLFactory.factory(rf_classifier)\n",
    "pipe_ml = pm.MLTrainTest(experiment_path, pickle_data_file_name, classifier=cls2)\n",
    "pipe_ml.fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e1a3b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
